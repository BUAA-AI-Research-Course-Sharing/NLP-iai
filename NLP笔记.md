<center><font size="10"><b>
   自然语言处理——NLP 
    </font>
</center>
* 悟道，中国智源
* 盘古：华为
* GPT3

# 第一章 绪论

工具 : Gensim

### **一 . 考核方式**:

- 平时分:10%

- 平时作业:15%*3 

- 大作业45%(自动文摘生成)

### **二. 课程主要内容** (共16章)

第1章 :绪论
第2章 数学基础 
第3章 形式语言与自动机 
第4章 语料库 
第5章 N元语法及其神经语言模型 
第6章 隐马尔可夫模型 
第7章 词法分析与词性标注 
第8章 语法理论 
第9章 句法分析
第10章 语义分析 
第11章 神经网络 
第12章 机器翻译 
第13章 文本分类与情感分类 
第14章 信息检索与问答系统 
第15章 自动文摘与信息抽取 
第16章 文本生成

### **三.NLP基本概念**

- 语言学 vs 语音学

- 自然语言理解 vs 自然语言处理 vs 计算机语言学 vs 中文信息处理

 

- **三个不同的语系**: 
  **屈折语**：用词的形态变化表示语法关系，如英语、法语等 
  **黏着语**：词内有专门表示语法意义的附加成分，词根或词干与附加成分的结合不紧密，如日语、韩语、土耳其语等 
  **孤立语**：形态变化少，语法关系靠词序和虚词表示

### 四.研究内容

1. **机器翻译**（Machine translation, MT）：实现一种语言到另一种语言的自动翻译

2. **信息检索**(Information retrieval) :信息检索也称情报检索，就是利用计算机系统从大量文档中找到符合用户需要的相关信息
3. **自动文摘**(Automatic summarization / Automatic abstracting) :将原文档的主要内容或某方面的信息自动提取出来，并形成原文档的摘要或缩写;即观点挖掘
4. **问答系统**(Question-answering system):通过计算机系统对人提出的问题的理解，利用自动推理等手段，在有关知识资源中自动求解答案并做出相应的回答。问答技术有时与语音技术和多模态输入/输出技术，以及人机交互技术等相结合，构成人机对话系统.
5. **社区问答**(Community Question Answering, CQA) :
6. ………………………………………………………………………..等



### 五.基本问题和主要困难

#### (一)困难一:大量的歧义

1. **词性歧义**:<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210916235537601.png" alt="image-20210916235537601" style="zoom:33%;" />
2. **结构歧义**:<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210916235511944.png" alt="image-20210916235511944" style="zoom:33%;" />
1. **语义歧义**:<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210916235608914.png" alt="image-20210916235608914" style="zoom:33%;" />
2. **语音歧义**:<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210916235621396.png" alt="image-20210916235621396" style="zoom:33%;" />
3. **多音字及韵律等歧义**:<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210916235705629.png" alt="image-20210916235705629" style="zoom:33%;" />

#### (二)困难二:大量未知语言现象

1. **新词、人名、地名、术语等**，如:  裸退、非典、夏天、高山、 温馨、时光、吉林、不来梅、失联、布莱尔
2. **新含义**，如：苹果、奔腾、同志、小姐、老虎、苍蝇等 
3. **新用法和新句型等**：，尤其在口语中或部分网络语言中，不断出现一些“非规范的”新的语句结构。如：被长工资，很中国，百度一下 

#### (三)归纳起来NLU所面临的挑

- **普遍存在的不确定性**：词法、句法、语义、语用和语音各个层面 
- **未知语言现象的不可预测性**：新的词汇、新的术语、新的语义和语法无处不在 
- **始终面临的数据不充分性**：有限的语言集合永远无法涵盖开放的语言现象 
- **语言知识表达的复杂性**：语义知识的模糊性和错综复杂的关联性难以用常规方法有效地描述，为语义计算带来了极大的困难

# 第二章 数学基础

1. 概率论
2. 信息论

3. 工神经网络

4. 应用举例

5. 习题与总结

###  **一. 概率论基础**

概率：反映事件出现的可能性大小

最大似然估计（maximum likehood estimation）:

贝叶斯决策理论（Bayesian decision theory）: 

- 已知类条件概率密度参数表达式和先验概率
- 利用贝叶斯公式转换成后验概率
- 根据后验概率大小进行决策分类

贝叶斯法则（公式）:

二项分布:



### **二.信息论基础**

1. **信息熵**（entropy）：如果X是一个离散性随机变量，其概率分布为：p(x)=P(X=x),x $\in$ X 。**X的熵H(X)定义为**：

$$
H(X)=-\sum_{x\in X}p(x)log_2p(x)
$$
​		其中，约定0log0=0;H(X)也可以写为H(p)，通常熵的单位为二进制位（bit）
* **熵的物理意义**:熵又称为自信息(self-information)，表示信源 X 每发一个符号(不论发什么符号)所提供的平均信息量。熵也可以被视为描述一个随机变量的不确定性的数量。 **一个随机变量的熵越大，它的不确定性越大。那么，正确估计其的可能性就越小。越不确定的随机变量越需要大的信息量用以确定其值**,熵值为表示这个信息最少需要的信息量(通常是bit)。



2. **联合熵:**如果X,Y是一对离散型随机变量，X,Y$\sim$p(x,y) 。**X,Y的熵H(X,Y)为**：
$$
H(X,Y)=-\sum_{x\in X}\sum_{y\in Y} p(x,y)log_2p(x,y)
$$

- 联合熵实际上就是描述一对随机变量平均所需要的信息量。

3. **条件熵**:  
   $$
   \begin{align}
   H(Y|X)&=\sum_{x\in X}p(x)H(Y|X=x)\\
   &= \sum_{x \in X}p(x)[-\sum_{y\in Y}p(y|x)log_2p(y|x)] \tag{3} \\
   &=-\sum_{x\in X}\sum_{y\in Y} p(x,y)log_2p(x,y) \tag{3}
   \end{align}
   $$
   
4. 

- **连锁规则:H(X,Y)=H(X)+H(Y|X)**

4. **相对熵：**（relative entrop 或 Kullback-Leibler divergence KL距离）

   两个概率分布p(x)和q(x)的**相对熵定义为**：
   $$
   D(p||q)=\sum_{x\in X}p(x) \log\frac{p(x)}{q(x)}
   $$
   约定:0log(0/q)=0;plog(p/0)=$\infin$

*    **相对熵的物理意义**：相对熵常被用以衡量两个随机分布的差距。当两个随机分布相同时，其相对熵为0。当两个随机分布的差别增加时，其相对熵也增加。<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210921014858661.png" alt="image-20210921014858661" style="zoom:50%;" />

5. **交叉熵(cross entropy):**如果一个随机变量X$\sim$p(x),q(x)为用于近似p(x)的概率分布,那么,随机变量X和模型q之间的交叉熵定义为:

$$
\begin{align}
H(X,q)&=H(X)+D(p||q) \\
	&=-\sum_{x\in X}p(x)\log q(x)\tag{5}
\end{align}
$$

- **交叉熵的概念用以衡量估计模型与真实概率分布之间的差异**。

6. **困惑度**：在设计语言模型时,我们通常用困惑度代替交叉熵语言模型的好坏。给定语言L的样本$l^n_1=l_1, \cdots l_n$,L的困惑度$PP_q$的定义为：
   $$
   PP_d=2^{H(l,q)}\approx 2^{-\frac{1}{n}\log q(l^n_1)} = [q(l^n_1)]^{-\frac{1}{n}}
   $$
   语言模型设计的任务就是寻找困惑度最小的模型，使其最接近真实的语言。

7. **互信息**（mutual information）： 如果$（X,Y）\sim p(x,y),X,Y$之间的互信息$I(X;Y)$定义为：
   $$
   I(X;Y)=H(X)-H(X|Y)
   $$
   根据$H(X)$和$$H(X|Y)$的定义

$$
I(X;Y)= \sum_{x\in X}\sum_{y\in Y}p(x,y)log_2\frac{p(x,y)}{p(x)p(y)}
$$

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210921134430075.png" alt="image-20210921134430075" style="zoom:67%;" />



#### 互信息，条件熵与联合熵的关系

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210921132806326.png" alt="image-20210921132806326" style="zoom:67%;" />

**噪声信道模型：**（了解）

 

### 三.人工神经网络基础

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210921135119364.png" alt="image-20210921135119364" style="zoom:80%;" />

**1. BP算法**：（误差反向传播算法，error Back Propagation） 

- BP算法中核心的数学工具就是微积分的**链式求导法则** 
- **算法步骤**： 
  1. 输入样本、学习率 
  2.  初始化权重W，与偏置b 
  3. 反复执行：(1)正向传播信息：选定样本，沿着一层层的网络算出估计值y（2）反向传播误差：依照估计值与实际值，由损失函数产生的梯 度，更新W,   b 

![image-20210921142254618](E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210921142254618.png)

### 四.应用举例

#### **词汇歧义消解**：

##### 基于上下文的消歧方法

1. **基于贝叶斯分类器**：
2. **基于最大熵的消岐**

 

 

# 第三章 语言

### 一.形式语言

**形式语言的定义**：形式语言是用来精确地描述语言（包括人工语言和自然语言）及其结构的手段形式语言学也称代数语言学.

**形式语法的定义**：形式语法是一个4元组G=(N,$\sum$,P,S),  其中**$N$是非终结符**的有限集合(有时也叫变量集或句法种类集);**$\sum$是终结符**的有限集合，$N\cap\sum=\emptyset$ ;$V=N\cup \sum$称为总词汇表；**$P$是一组重写规则**的有限集合：$P=\{\alpha \rightarrow \beta\}$}，其中，$\alpha,\beta$是$P$中元素构成的串。**$S\in N$，称为句子符或初始符**。

**推导的定义**：<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210921153649735.png" alt="image-20210921153649735" style="zoom:50%;" />

**最左推导、最右推导和规范推导**：

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210921153828990.png" alt="image-20210921153828990" style="zoom:50%;" />

  **句子和句型：**

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210921161700739.png" alt="image-20210921161700739" style="zoom:50%;" />

**正则文法（3型文法）**

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210921161802290.png" alt="image-20210921161802290" style="zoom:50%;" />

**上下文无关文法（2型文法）：**

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210921161942013.png" alt="image-20210921161942013" style="zoom:50%;" />

**上下文有关文法（1型文法）：**

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210921162038428.png" alt="image-20210921162038428" style="zoom:50%;" />

**无约束文法（0型文法）：**

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210921162511366.png" alt="image-20210921162511366" style="zoom:50%;" />

**产生的语言句子的派生树表示 :**

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210921162648363.png" alt="image-20210921162648363" style="zoom:50%;" />

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210921162656470.png" alt="image-20210921162656470" style="zoom:50%;" />

### 二.有限自动机与正则文法

1. **确定的有限自动机（DFA）:**

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210921194108376.png" alt="image-20210921194108376" style="zoom:67%;" />

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210921162813731.png" alt="image-20210921162813731" style="zoom:50%;" />

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210921192525451.png" alt="image-20210921192525451" style="zoom:50%;" />

2. **不确定有限自动机（NFA）：**

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210921192747431.png" alt="image-20210921192747431" style="zoom:50%;" />

3. **DFA和NFA区别与关系：**

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210921192947439.png" alt="image-20210921192947439" style="zoom:50%;" />

4. **正则文法与有限自动机的关系：**

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210921193054879.png" alt="image-20210921193054879" style="zoom:50%;" />

### 三.下推自动机与CFG

1. **下推自动机（PDA）**:可以看成一个带有附加的下推存储器的有限自动机，下推
   存储器是一个栈。如下图所示： <img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210921194025273.png" alt="image-20210921194025273" style="zoom: 75%;" />

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210921194150251.png" alt="image-20210921194150251" style="zoom:50%;" />

2. **语言与识别器的对应关系：**

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210921200105544.png" alt="image-20210921200105544" style="zoom:50%;" />

### 四.有限自动机在NLP中的应用

1. 英语单词拼写检查（计算两字符串编辑距离的具体算法）
2. 英语单词形态分析



# 第四章 语料库

### 一. 语料库的概念

1. 什么是语料库:语料库（corpus）一词在语言学上通常指经过整理，具有既定格式与标记的大量的文本。
2. 语料库的种类:
   - 共时语料库和历时语料库
   - 通用语料库和专用语料库
   - 生语料库和标注语料库:是否经过处理标注,根据加工程度不同标注语料库可以继续细分为:分词语料库$\cdots$等
3. 语料库语言学：基于语料库进行语言学的研究 
4. 语料库语言学研究的内容： 
   - 语料库的建设与编纂 
   - 语料库的加工和管理技术
   - 语料库的使用

### 二.语料库发展史

1. 三个阶段:

   - 早期(20世纪50年代中期之前):语料库在语言研究中被广泛使用：语言习得、方言学、语言教学、句法和语义、音系研究等
   - 沉寂时期(1957～20世纪80年代初期)
   - 复苏与发展时期(20世纪80年代以后):第二代语料库相继建成;基于语料库的研究项目增多;

2. 国内语料库现状

   * 北航现代汉语语料库(1983年，2000万字)
   * 武汉大学汉语现代文学作品语料库(1979年,527万字)
   * 北语现代汉语词频统计语料库(1983年,182万字)。目前北京语言大学正面向“一带一路”战略开展语料库研究和开发工作  
   * 北师大中学语文教材语料库(1983年,106万字)
   * 清华汉语歧义切分语料库(1998年,1亿汉字)，后来在汉语树库、篇章语料库建设等方面做了大量研发工作
   * 北大:[北大渝士汶教授综合性语言知识库CLBK](http://icl.pku.edu.cn/)

   - 台湾中研院:Sinica Corpus

### 三.典型语料库

1. 典型语料库介绍
   * 北大:[北大渝士汶教授综合性语言知识库CLBK](http://icl.pku.edu.cn/)
* 中国台湾中研院平衡语料库:Sinica Corpus
   * 宾州(Pennsylvania)大学语料库 (UPenn Tree Bank)
* 朗文语料库 (Longman Corpus) :书面语:尊重本族语言者的直觉和语料库权威
   * Chinese LDC:国家973项目
* LC-STAR(NLPR-Nokia):14 国语言：英文、俄语、中文、西班牙语 ... ;文本语料不少于100M words （中文约3000万字）
   * [国家语委语料库](http://corpus.china-language.edu.cn/)
* <img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210928214232010.png" alt="image-20210928214232010" style="zoom:50%;" />
   * [北京大学计算语言学研究所](http://www.icl.pku.edu.cn/icl_res/)
* [哈工大信息检索研究室对外共享语料库资源](http://ir.hit.edu.cn/demo/ltp/Sharing_Plan.htm)
   * [香港教育学院__语言资讯科学中心及其语料库实验室](http://www.livac.org/index.php?lang=sc)

### 四.语料库加工方法

1. 文本处理
   - 垃圾格式问题:需要过滤器来过滤杂质
   - 大小写问题
   - 标记化(tokenization)
   - 句点
   - 连字符
   - 相同形式表示不同语义的词语
   - 词法:词干化(stemming)
   - 句子定义:
   - 句子边界研究
2. 格式标注
   - 句通用标记语言(SGML):例如:HTML和XML
3. 数据标注
- 语法标注 
4. 搭配抽取
   - 频率方法:按照两个词一起出现的频率
   - 均值和方差方法:寻找两个词大致以相同的距离出现(低方差值)

### 五.扩展:词汇知识库

1. [**WordNet**](http://wordnet.princeton.edu/):
- 包含:名词、动词、形容词、副词、虚词(实际上不包含虚词)
  
- 四种语义关系：同义关系，反义关系，上下位关系，部分关系
  
2. **HowNet（知网）**



# 第五章 N元语法

### 一.基本概念

- **如何计算一段文字（句子）的概率?**

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210929142327818.png" alt="image-20210929142327818" style="zoom: 67%;" />![image-20210929142344955](E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210929142344955.png)

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210929142443361.png" alt="image-20210929142443361" style="zoom:67%;" />

2. **N元语法模型**

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210929142540158.png" alt="image-20210929142540158" style="zoom:67%;" />![image-20210929142555314](E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210929142555314.png)

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210929142540158.png" alt="image-20210929142540158" style="zoom:67%;" />![image-20210929142555314](E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210929142555314.png)

### 二.参数估计

1. **训练语料**:用于建立模型，确定模型参数的已知语料。

2. **最大似然估计**：用相对频率计算概率的方法。

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210929142941081.png" alt="image-20210929142941081" style="zoom:67%;" />

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210929142949397.png" alt="image-20210929142949397" style="zoom:67%;" />

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210929143006275.png" alt="image-20210929143006275" style="zoom:67%;" />



### 三.数据平滑

1. **数据平滑的基本思想**：调整最大似然估计的概率值，使零概率增值，是非零概率下调，“劫富济贫”，消除零概率，改进模型的整体正确率。
2. **基本目标：**测试样本语言模型困惑度越小越好
3. **基本约束：**$\sum_{w_i}p(w_i|w_1,w_2,\cdots ,w_{i-1})=1$

4. **数据平滑方法：**
   
   - **加1法：**<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20210929143502601.png" alt="image-20210929143502601" style="zoom:67%;" />
   
   - **减值法/折扣法(discounting)**
   
     **基本思想**:修改训练样本种事件的实际计数,使样本中(实际出现的)不同事件的概率之和小于1,剩余的概率量分配给未见概率.

### 四.语言模型自适应





### 五. 语言模型应用





# 第六章 隐形马尔可夫模型

### 一.马尔可夫(markov)模型

1. 满足：$p(q_t=S_j|q_{t-1}=S_i,q_{t-2}=S_k,\cdots)=p(q_t=S_j|q_{t-1}=S_i)=a_{ij}$的模型称为**马尔可夫模型**
2. **状态序列的概率:** $p(S_1,\cdots,S_T)=p(S_1)p(S_2|S_1)p(S_3|S_2)\cdots$

3. 马尔可夫模型又可视为随机的有限状态自动机，该 有限状态自动机的每一个状态转换过程都有一个相应的概率，该概率表示自动机采用这一状态转换的可能性。

<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20211028112829182.png" alt="image-20211028112829182" style="zoom:67%;" />

### 二.隐形马尔可夫模型

1. 隐形马尔可夫模型是一个**双重随机过程,**只知道状态转移的概率,即模型的状态转换过程是不可观察的(隐蔽的),而可观察的事件的随机过程是隐蔽状态转 换过程的随机函数。

2. **图解HMM:**

   <img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20211028112920209.png" alt="image-20211028112920209" style="zoom:67%;" />

3. **隐形马尔可夫模型的5个参数**

   - N：状态数
   - M：可能的输出数目
   - $A=A_{ij}$,状态转移矩阵
   - $B=b_i(k)$,$k=1,2,\cdots N$在每个状态的输出概率分布
   - $\pi=\pi_i$初始状态概率分布

4. **给定HMM求观察序列**

   <img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20211028113250578.png" alt="image-20211028113250578" style="zoom:67%;" />

5. **HMM主要可以解决以下三个问题**
   * (1) 在给定模型$\mu=(A, B, \pi)$ 和观察序列 $O＝O_1,O_2  ...O_T$ 的情况下，怎样快速计算概率 $p(O|\mu)$？ (2)
   * (2) 在给定模型 $\mu=(A, B, \pi)$和观察序列  $O＝O_1,O_2  ...O_T$ 的情况下，如何选择在一定意义下“最优”的状态序列 $Q = q_1,q_2...q_T$，使得该状态序列“最好地解 释”观察序列？ 
   * (3) 给定一个观察序列 $O＝O_1,O_2,...,O_T$ ，如何根据最大 似然估计来求模型的参数值？即如何调节模型的参数，使得P$(O|\mu)$ 最大？

### 三. 前向算法

* 在给定模型$\mu=(A, B, \pi)$ 和观察序列 $O＝O_1,O_2  ...O_T$ 的情况下，快速计算概率 $p(O|\mu)$



3. **算法时间复杂度$O(N^2T)$**

 

### 四.后向算法





### 五.Viterbi搜索算法





### 六.参数学习







### 七.应用举例







### 八。CRFs及其应用



# 第七章 词法分析和词性标注

### 一,概述

1. 不同语言的词法分析:

   * 曲折语(如:英语,德语,俄语等)
   * 分析语/孤立语(如:汉语)

   * 黏着语(如:日语)
2. **词性标注:**又称为词类标注或者简称标注，是指为分词结果中的每个单词标注一个正确的词性的程序，也即确定每个词是名词、动词、形容词或者其他词性的过程。
3. **命名实体识别：**是在句子的词序列中定位并识别人名、地名、机构名等实体的任务。属于词法分析中未登录词识别的范畴
   * 通常包含两部分任务：
     * 实体边界识别； 
     * 确定实体类别（人名、地名、机构名或其他）

### 二.英语的形态分析

1. **基本任务:**
* **单词识别**
   * **形态还原**
* 有规律变化单词的形态还原
     * 动词、名词、形容词、副词不规则变化单词的形态还原建立不规则变化词表
  * 对于表示年代、时间、百分数、货币、序数词的数字形态还原
     * 合成词的形态还原

2. 形态分析的一般方法 
   * 查词典，如果词典中有该词，直接确定该词的原形； 
   * 根据不同情况查找相应规则对单词进行还原处理，如果
     还原后在词典中找到该词，则得到该词的原形；如果找
     不到相应变换规则或者变换后词典中仍查不到该词，则
     作为未登录词处理； 
   * 进入未登录词处理模块。

### 三.汉语自动分词概要

1. 汉语自动分词的重要性 
   * 自动分词是汉语句子分析的基础 
   * 词语的分析具有广泛的应用（词频统计，词典编纂，文章风格研究等） 
   * 文献处理以词语为文本特征 
   * “以词定字、以词定音”，用于文本校对、同音字识别、多音字辨识、简繁体转换

2. **汉语自动分词中的主要问题** 
   * 汉语分词规范问题(《信息处理用限定汉语分词规范（
     GB13715）》)       
   * －汉语中什么是词？两个不清的界限： 
     * (1) 单字词与词素，如：新华社25日讯 
     * (2) 词与短语，如：花草，湖边，房顶，鸭蛋，小鸟，担水，一层 

### 四.分词歧义

1. **交集型切分歧义**
   * 汉字串AJB被称作交集型切分歧义，如果满足AJ、JB同时为词(A、J、B分别为汉字串)。此时汉字串J被称作交集串
   * 链长定义：一个交集型切分歧义所拥有的交集串的集合称为交集串链，它的个数称为链长。

2. **组合型切分歧义** 
   * 汉字串AB被称作组合型切分歧义，如果满足条件：A、 B、AB同时为词
3. **真实文本中分词歧义的分布情况** 
   * **交集型歧义：组合型歧义 = 1：22**   预料规模：17,547字 [1] 预料规模：500万字新闻预料 [2] 
   * **真歧义”和“伪歧义”** 
     * 真歧义指存在两种或两种以上的可实现的切分形式，        
       *  如句子“必须/加强/企业/中/国有/资产/的/管理/”和“中 国/有/能力/解决/香港/问题/”中的字段“中国有”是一种真歧义 
     * 伪歧义一般只有一种正确的切分形式，          
       * 如“建设/有”、 “中国/人民”、“各/地方”、 “本/地区”等
   * ~<img src="E:\univeisity\大三上\NLP\NLP笔记.assets\image-20211028105646940.png" alt="image-20211028105646940" style="zoom:67%;" />

### 五.未登录词











# 第八章 语法分析

### 1.概述

1. 描述自然语言的结构和语义



### 2.功能合一语法FUG

1. **提出起因**:

   1. 短语结构语法生成能力太强,产生许多不符合语法或有歧义的句子。
   2. 标记十分简单，分析有限，难以反应复杂的自然语言

2. FUG对短语结构语法的改进。

   1.  。。。

   2. 。。。

3. **复杂特征集：**嵌套结构
4.  **合一运算:**





### 3.词汇功能语法

1. 基本观点：句子由两个相对独立的层次来描述的
   1. 成分结构：
   2. 功能结构：
2. 要点：
   1. 突出词汇的作用，体现”语法结构可以由某些词的意义预示出来“基本观点
   2. 把功能结构的描述作为语言描述中的一个基本的独立层次
   3. 
3. 两个基本作用
   1. 可以准确地解释语言现象
   2. 可以减轻语法规则的作用
4. LFG的两个语法层次结构
   1. 成分结构（C-结构）
   2. 功能结构（F-结构）
      1. f描述是一系列等式的集合
5. 代真过程的合法性
6. 从f描述到F结构：



7. 功能合格条件



8. 完备性条件
9. 关联性条件
10. 词汇功能语法的特点





### 4. 本章小结

1. 功能和一语法(FUG)

   * 对短语结构语法的改进
   * 复杂特征集

   * 合一运算

2. 词汇功能语法（FUG）

   * 基本观点、特点
   * 两个语法层次结构
   * 由C结构经F描述道F结构





### 5.扩展

1. **GB理论：**

   

# 第九章 句法分析





# 第十章 语义分析







# 第十四章 自动文摘生成

## 一. 抽取式方法

### 1. 基于统计和规则的文摘方法

![image-20211219142028659](E:\univeisity\大三上\NLP\NLP笔记.assets\image-20211219142028659.png)



### 2. 基于图模型的文摘方法

![image-20211219142052309](E:\univeisity\大三上\NLP\NLP笔记.assets\image-20211219142052309.png)



### 3. 基于主题的文摘方法

![image-20211219142222424](E:\univeisity\大三上\NLP\NLP笔记.assets\image-20211219142222424.png)



### 4. 基于整数规划的文摘方法

![image-20211219142256766](E:\univeisity\大三上\NLP\NLP笔记.assets\image-20211219142256766.png)



## 二. 生成式方法

![image-20211219143213294](E:\univeisity\大三上\NLP\NLP笔记.assets\image-20211219143213294.png)

![image-20211219143223328](E:\univeisity\大三上\NLP\NLP笔记.assets\image-20211219143223328.png)

### 1. 基于深度学习的文摘方法

![image-20211219143424054](E:\univeisity\大三上\NLP\NLP笔记.assets\image-20211219143424054.png)